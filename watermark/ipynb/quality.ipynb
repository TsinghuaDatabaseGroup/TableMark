{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "pd.options.display.float_format = '{:.3g}'.format\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_columns = 100\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "df_real_mle = pd.read_json('../results_final/real_mle.json', lines=True)\n",
    "df_real_mle = df_real_mle.groupby('dataname', as_index=True)[['auroc', 'rmse']].agg('mean')\n",
    "\n",
    "df = pd.read_json('../results_final/quality.json', lines=True)\n",
    "df = df[df['embedding_model'] == 'original-num_layers2-final']\n",
    "df_watermark = df[df['watermark'] == 'pair_compare_one_pair']\n",
    "df_non_watermark = df[df['watermark'] == 'none']\n",
    "\n",
    "df_freqwm = pd.read_json('../results_final/freqwm_quality.json', lines=True)\n",
    "df_freqwm = df_freqwm.rename(columns={\n",
    "  'num_watermark_bits': 'bits',\n",
    "  'min_hamming_dist': 'hamming_distance'\n",
    "})\n",
    "\n",
    "df_tabular_mark = pd.read_json('../results_final/tabular_mark_quality.json', lines=True)\n",
    "df_tabular_mark = df_tabular_mark[df_tabular_mark['num_cells_ratio'] == 0.15]\n",
    "df_tabular_mark = df_tabular_mark[df_tabular_mark['p_ratio'] == 0.2]\n",
    "df_tabular_mark = df_tabular_mark[df_tabular_mark['num_units'] == 2]\n",
    "df_tabular_mark = df_tabular_mark.rename(columns={\n",
    "  'num_watermark_bits': 'bits',\n",
    "  'min_hamming_dist': 'hamming_distance'\n",
    "})\n",
    "\n",
    "df_tabwak_partition = pd.read_json('../results_final/tabwak_partition_quality.json', lines=True)\n",
    "df_tabwak_partition = df_tabwak_partition.rename(columns={\n",
    "  'num_watermark_bits': 'bits',\n",
    "  'min_hamming_dist': 'hamming_distance'\n",
    "})\n",
    "\n",
    "all_datanames = [\n",
    "  'beijing',\n",
    "  'default',\n",
    "  'phishing',\n",
    "  'shoppers',\n",
    "]\n",
    "all_num_users = [\n",
    "  1000,\n",
    "]\n",
    "all_watermark = [\n",
    "  'pair_compare_one_pair',\n",
    "]\n",
    "all_num_classes = [\n",
    "  256,\n",
    "]\n",
    "all_num_watermark_bits = [\n",
    "  32,\n",
    "]\n",
    "all_min_hamming_dist = [\n",
    "  7,\n",
    "]\n",
    "all_max_num_error_bits = [\n",
    "  3,\n",
    "]\n",
    "all_ratio_num_samples_per_class_interval = [\n",
    "  -30000,\n",
    "]\n",
    "all_quality_loss = [\n",
    "  'quad_random_init985',\n",
    "]\n",
    "all_embedding_models = [\n",
    "  'orig',\n",
    "]\n",
    "all_quality_modes = [\n",
    "  'average',\n",
    "]\n",
    "all_gen_code_losses = [\n",
    "  'general_bfs',\n",
    "]\n",
    "all_tao_approximations = [\n",
    "  0\n",
    "]\n",
    "all_time_limits = [\n",
    "  180,\n",
    "]\n",
    "all_upper_bounds = [\n",
    "  '-1',\n",
    "  '1stage_final-0.01-0.01',\n",
    "  '1stage_final-0.001-0.001',\n",
    "  '6stage_splus-0.001-0.001',\n",
    "  '6stage_splus-0.01-0.01',\n",
    "  '6stage_splus-0.1-0.1',\n",
    "]\n",
    "all_error_rates = [\n",
    "  0.001,\n",
    "]\n",
    "all_gauss = [\n",
    "  0.01,\n",
    "]\n",
    "all_alter = [\n",
    "  0.01,\n",
    "]\n",
    "all_deletion_rates = [\n",
    "  0.1,\n",
    "]\n",
    "all_dim_ratios = [\n",
    "  'correct-pca-0.99',\n",
    "]\n",
    "all_norms = [\n",
    "  0,\n",
    "]\n",
    "all_metric = [\n",
    "  'quad_loss',\n",
    "  'shape',\n",
    "  'trend',\n",
    "  'detection',\n",
    "  'auroc',\n",
    "  'rmse',\n",
    "  'count0.01_95th',\n",
    "  'avg0.01_95th',\n",
    "  'count0.05_95th',\n",
    "  'avg0.05_95th',\n",
    "  'count0.2_95th',\n",
    "  'avg0.2_95th',\n",
    "]\n",
    "all_metric_non_watermark = all_metric[1:] if 'quad_loss' in all_metric else all_metric\n",
    "\n",
    "df_non_watermark = df_non_watermark[df_non_watermark['dataname'].isin(all_datanames)]\n",
    "df_non_watermark = df_non_watermark[df_non_watermark['num_classes'].isin(all_num_classes)]\n",
    "df_non_watermark = df_non_watermark[df_non_watermark['dim_ratio'].isin(all_dim_ratios)]\n",
    "df_non_watermark = df_non_watermark[['dataname', 'num_classes', 'num_samples', 'dim_ratio'] + all_metric_non_watermark]\n",
    "\n",
    "df_watermark.loc[df_watermark['num_samples_per_class_upper_bound'] == '1stage_sym-0.1', 'num_samples_per_class_upper_bound'] = '1stage_final-0.1-0.1'\n",
    "df_watermark.loc[pd.isna(df_watermark['norm']), 'norm'] = False\n",
    "df_watermark = df_watermark[df_watermark['watermark'] != 'none']\n",
    "df_watermark['embedding_model'] = df_watermark['embedding_model'].apply(lambda x: x[:4])\n",
    "df_watermark = df_watermark[df_watermark['embedding_model'].isin(all_embedding_models)]\n",
    "df_freqwm['embedding_model'] = df_freqwm['embedding_model'].apply(lambda x: x[:4])\n",
    "df_freqwm = df_freqwm[df_freqwm['embedding_model'].isin(all_embedding_models)]\n",
    "df_tabular_mark['embedding_model'] = df_tabular_mark['embedding_model'].apply(lambda x: x[:4])\n",
    "df_tabular_mark = df_tabular_mark[df_tabular_mark['embedding_model'].isin(all_embedding_models)]\n",
    "df_tabwak_partition['embedding_model'] = df_tabwak_partition['embedding_model'].apply(lambda x: x[:4])\n",
    "df_tabwak_partition = df_tabwak_partition[df_tabwak_partition['embedding_model'].isin(all_embedding_models)]\n",
    "df_watermark = df_watermark[df_watermark['time_limit'].isin(all_time_limits)]\n",
    "df_watermark = df_watermark[df_watermark['dataname'].isin(all_datanames)]\n",
    "df_freqwm = df_freqwm[df_freqwm['dataname'].isin(all_datanames)]\n",
    "df_tabular_mark = df_tabular_mark[df_tabular_mark['dataname'].isin(all_datanames)]\n",
    "df_tabwak_partition = df_tabwak_partition[df_tabwak_partition['dataname'].isin(all_datanames)]\n",
    "df_watermark = df_watermark[['dataname', 'embedding_model', 'num_users', 'watermark', 'num_classes', 'num_samples', 'num_watermark_bits', 'min_hamming_dist', 'max_num_error_bits', 'ratio_num_samples_per_class_interval', 'quality_loss', 'tao_approximation', 'bit_error_rate', 'deletion_rate', 'time_limit', 'quality_mode', 'error_rate', 'gen_code_loss', 'num_samples_per_class_upper_bound', 'min_gap', 'dim_ratio',\n",
    "                             'norm', 'cluster_algorithm', 'loss', 'gauss', 'alter',\n",
    "                             ] + all_metric]\n",
    "df_watermark = df_watermark[df_watermark['num_users'].isin(all_num_users)]\n",
    "df_freqwm = df_freqwm[df_freqwm['num_users'].isin(all_num_users)]\n",
    "df_tabular_mark = df_tabular_mark[df_tabular_mark['num_users'].isin(all_num_users)]\n",
    "df_tabwak_partition = df_tabwak_partition[df_tabwak_partition['num_users'].isin(all_num_users)]\n",
    "df_watermark = df_watermark[df_watermark['watermark'].isin(all_watermark)]\n",
    "df_watermark = df_watermark[df_watermark['num_classes'].isin(all_num_classes)]\n",
    "df_freqwm = df_freqwm[df_freqwm['num_classes'].isin(all_num_classes)]\n",
    "df_tabular_mark = df_tabular_mark[df_tabular_mark['num_classes'].isin(all_num_classes)]\n",
    "df_watermark = df_watermark[df_watermark['num_watermark_bits'].isin(all_num_watermark_bits)]\n",
    "df_freqwm = df_freqwm[df_freqwm['bits'].isin(all_num_watermark_bits)]\n",
    "df_tabular_mark = df_tabular_mark[df_tabular_mark['bits'].isin(all_num_watermark_bits)]\n",
    "df_tabwak_partition = df_tabwak_partition[df_tabwak_partition['bits'].isin(all_num_watermark_bits)]\n",
    "df_watermark = df_watermark[df_watermark['min_hamming_dist'].isin(all_min_hamming_dist)]\n",
    "df_watermark = df_watermark[df_watermark['ratio_num_samples_per_class_interval'].isin(all_ratio_num_samples_per_class_interval)]\n",
    "df_watermark = df_watermark[df_watermark['quality_loss'].isin(all_quality_loss)]\n",
    "df_watermark = df_watermark[df_watermark['quality_mode'].isin(all_quality_modes)]\n",
    "df_watermark = df_watermark[df_watermark['gen_code_loss'].isin(all_gen_code_losses)]\n",
    "df_watermark = df_watermark[df_watermark['num_samples_per_class_upper_bound'].isin(all_upper_bounds)]\n",
    "df_watermark = df_watermark[df_watermark['max_num_error_bits'].isin(all_max_num_error_bits)]\n",
    "df_watermark = df_watermark[df_watermark['error_rate'].isin(all_error_rates)]\n",
    "df_freqwm = df_freqwm[df_freqwm['max_num_error_bits'].isin(all_max_num_error_bits)]\n",
    "df_tabular_mark = df_tabular_mark[df_tabular_mark['max_num_error_bits'].isin(all_max_num_error_bits)]\n",
    "df_tabwak_partition = df_tabwak_partition[df_tabwak_partition['max_num_error_bits'].isin(all_max_num_error_bits)]\n",
    "df_watermark = df_watermark[df_watermark['deletion_rate'].isin(all_deletion_rates)]\n",
    "df_watermark = df_watermark[df_watermark['dim_ratio'].isin(all_dim_ratios)]\n",
    "df_freqwm = df_freqwm[df_freqwm['dim_ratio'].isin(all_dim_ratios)]\n",
    "df_tabular_mark = df_tabular_mark[df_tabular_mark['dim_ratio'].isin(all_dim_ratios)]\n",
    "df_watermark = df_watermark[df_watermark['norm'].isin(all_norms)]\n",
    "df_freqwm = df_freqwm[df_freqwm['norm'].isin(all_norms)]\n",
    "df_tabular_mark = df_tabular_mark[df_tabular_mark['norm'].isin(all_norms)]\n",
    "df_watermark = df_watermark[df_watermark['gauss'].isin(all_gauss)]\n",
    "df_watermark = df_watermark[df_watermark['alter'].isin(all_alter)]\n",
    "df_watermark = df_watermark.rename(columns={\n",
    "  'ratio_num_samples_per_class_interval': 'ratio',\n",
    "  'num_watermark_bits': 'bits',\n",
    "  'min_hamming_dist': 'hamming_distance'\n",
    "})\n",
    "\n",
    "df_watermark_failed = df_watermark[(df_watermark['quad_loss'] == -1) | (df_watermark['auroc'] == -1) | (df_watermark['rmse'] == -1)]\n",
    "df_watermark = df_watermark[(df_watermark['quad_loss'] != -1) & (df_watermark['auroc'] != -1) & (df_watermark['rmse'] != -1)]\n",
    "\n",
    "df_watermark['quad_loss'] = df_watermark['quad_loss'] ** 0.5\n",
    "def process_row(row: pd.Series):\n",
    "  row['auroc'] = df_real_mle.loc[row['dataname'], 'auroc'] - row['auroc']\n",
    "  row['rmse'] = row['rmse'] - df_real_mle.loc[row['dataname'], 'rmse']\n",
    "  return row\n",
    "df_non_watermark = df_non_watermark.apply(process_row, axis=1)\n",
    "df_watermark = df_watermark.apply(process_row, axis=1)\n",
    "df_freqwm = df_freqwm.apply(process_row, axis=1)\n",
    "df_tabular_mark = df_tabular_mark.apply(process_row, axis=1)\n",
    "df_tabwak_partition = df_tabwak_partition.apply(process_row, axis=1)\n",
    "agg = {\n",
    "  all_metric[0]: [\n",
    "    'count',\n",
    "    'mean',\n",
    "  ]\n",
    "}\n",
    "agg_non_watermark = {\n",
    "  all_metric_non_watermark[0]: [\n",
    "    'count',\n",
    "    'mean',\n",
    "  ]\n",
    "}\n",
    "agg.update({all_metric[i]: [\n",
    "  'mean',\n",
    "] for i in range(1, len(all_metric))})\n",
    "agg_non_watermark.update({all_metric_non_watermark[i]: [\n",
    "  'mean',\n",
    "] for i in range(1, len(all_metric_non_watermark))})\n",
    "as_index = True\n",
    "df_non_watermark['shape'] = 1 - df_non_watermark['shape']\n",
    "df_non_watermark['trend'] = 1 - df_non_watermark['trend']\n",
    "df_non_watermark['detection'] = 1 - df_non_watermark['detection']\n",
    "df_non_watermark = df_non_watermark.groupby(by=['dataname', 'num_classes', 'num_samples', 'dim_ratio'], as_index=as_index, sort=True).head(100) \\\n",
    "                                   .groupby(by=['dataname', 'num_classes', 'num_samples', 'dim_ratio'], as_index=as_index, sort=True)[all_metric_non_watermark].agg(agg_non_watermark)\n",
    "\n",
    "df_watermark['shape'] = 1 - df_watermark['shape']\n",
    "df_watermark['trend'] = 1 - df_watermark['trend']\n",
    "df_watermark['detection'] = 1 - df_watermark['detection']\n",
    "df_watermark = df_watermark.groupby(by=['dataname', 'num_classes', 'bits', 'hamming_distance', 'watermark', 'quality_mode', 'error_rate', 'gauss', 'alter', 'deletion_rate',\n",
    "                                        'num_samples_per_class_upper_bound',\n",
    "                                        'cluster_algorithm',\n",
    "                                        'dim_ratio',\n",
    "                                        'norm',\n",
    "                                        'gen_code_loss',\n",
    "                                        ], as_index=as_index, sort=True).head(100).groupby(by=['dataname', 'num_classes', 'bits', 'hamming_distance', 'watermark', 'quality_mode', 'error_rate', 'gauss', 'alter', 'deletion_rate',\n",
    "                                        'num_samples_per_class_upper_bound',\n",
    "                                        'cluster_algorithm',\n",
    "                                        'dim_ratio',\n",
    "                                        'norm',\n",
    "                                        'gen_code_loss',\n",
    "                                        ], as_index=as_index, sort=True)[all_metric].agg(agg)\n",
    "\n",
    "df_watermark_failed = df_watermark_failed.groupby(by=['dataname', 'num_classes', 'bits', 'hamming_distance', 'watermark', 'quality_mode', 'error_rate', 'gauss', 'alter', 'deletion_rate',\n",
    "                                        'num_samples_per_class_upper_bound',\n",
    "                                        'cluster_algorithm',\n",
    "                                        'dim_ratio',\n",
    "                                        'norm',\n",
    "                                        'gen_code_loss',\n",
    "                                        ], as_index=as_index, sort=True)['quad_loss'].agg('count')\n",
    "\n",
    "df_freqwm['shape'] = 1 - df_freqwm['shape']\n",
    "df_freqwm['trend'] = 1 - df_freqwm['trend']\n",
    "df_freqwm['detection'] = 1 - df_freqwm['detection']\n",
    "df_freqwm = df_freqwm.groupby(by=['dataname', 'num_classes', 'bits', 'hamming_distance',\n",
    "                                  'cluster_algorithm',\n",
    "                                  'dim_ratio',\n",
    "                                  'norm',\n",
    "                                  ], as_index=as_index, sort=True).head(100).groupby(by=['dataname', 'num_classes', 'bits', 'hamming_distance',\n",
    "                                  'cluster_algorithm',\n",
    "                                  'dim_ratio',\n",
    "                                  'norm',\n",
    "                                  ], as_index=as_index, sort=True)[all_metric_non_watermark].agg(agg_non_watermark)\n",
    "\n",
    "df_tabular_mark['shape'] = 1 - df_tabular_mark['shape']\n",
    "df_tabular_mark['trend'] = 1 - df_tabular_mark['trend']\n",
    "df_tabular_mark['detection'] = 1 - df_tabular_mark['detection']\n",
    "df_tabular_mark = df_tabular_mark.groupby(by=['dataname', 'num_classes', 'bits', 'hamming_distance',\n",
    "                                  'cluster_algorithm',\n",
    "                                  'dim_ratio',\n",
    "                                  'norm',\n",
    "                                  ], as_index=as_index, sort=True).head(100).groupby(by=['dataname', 'num_classes', 'bits', 'hamming_distance',\n",
    "                                  'cluster_algorithm',\n",
    "                                  'dim_ratio',\n",
    "                                  'norm',\n",
    "                                  ], as_index=as_index, sort=True)[all_metric_non_watermark].agg(agg_non_watermark)\n",
    "df_tabwak_partition['shape'] = 1 - df_tabwak_partition['shape']\n",
    "df_tabwak_partition['trend'] = 1 - df_tabwak_partition['trend']\n",
    "df_tabwak_partition['detection'] = 1 - df_tabwak_partition['detection']\n",
    "df_tabwak_partition = df_tabwak_partition.groupby(by=['dataname', 'num_classes', 'bits', 'hamming_distance',\n",
    "                                                    'cluster_algorithm',\n",
    "                                                    'dim_ratio',\n",
    "                                                    'norm',\n",
    "                                                    ], as_index=as_index, sort=True).head(100).groupby(by=['dataname', 'num_classes', 'bits', 'hamming_distance',\n",
    "                                                    'cluster_algorithm',\n",
    "                                                    'dim_ratio',\n",
    "                                                    'norm',\n",
    "                                                    ], as_index=as_index, sort=True)[all_metric_non_watermark].agg(agg_non_watermark)\n",
    "\n",
    "for metric in all_metric_non_watermark:\n",
    "  df_non_watermark[metric] = df_non_watermark[metric].round(3)\n",
    "  df_freqwm[metric] = df_freqwm[metric].round(3)\n",
    "  df_tabular_mark[metric] = df_tabular_mark[metric].round(3)\n",
    "  df_tabwak_partition[metric] = df_tabwak_partition[metric].round(3)\n",
    "for metric in all_metric:\n",
    "  df_watermark[metric] = df_watermark[metric].round(3)\n",
    "\n",
    "display('non_watermark', df_non_watermark)\n",
    "display('TableMark', df_watermark)\n",
    "display('TableMark Failed', df_watermark_failed)\n",
    "display('FreqyWM', df_freqwm)\n",
    "display('TabularMark', df_tabular_mark)\n",
    "display('TabWak', df_tabwak_partition)\n",
    "\n",
    "df_watermark = df_watermark.reset_index()\n",
    "plt.figure(dpi=1000, figsize=(1.8 * len(all_datanames), 2.1))\n",
    "X = np.arange(len(all_datanames)) * 1.6\n",
    "y_no = df_watermark.loc[(df_watermark['num_samples_per_class_upper_bound'] == '-1') & (df_watermark['deletion_rate'] == 0.1) & (df_watermark['error_rate'] == 0.001) & (df_watermark['gauss'] == 0.01) & (df_watermark['alter'] == 0.01), ('quad_loss', 'mean')].to_list()\n",
    "y_simple01 = df_watermark.loc[(df_watermark['num_samples_per_class_upper_bound'] == '1stage_final-0.01-0.01') & (df_watermark['deletion_rate'] == 0.1) & (df_watermark['error_rate'] == 0.001) & (df_watermark['gauss'] == 0.01) & (df_watermark['alter'] == 0.01), ('quad_loss', 'mean')].to_list()\n",
    "y_simple001 = df_watermark.loc[(df_watermark['num_samples_per_class_upper_bound'] == '1stage_final-0.001-0.001') & (df_watermark['deletion_rate'] == 0.1) & (df_watermark['error_rate'] == 0.001) & (df_watermark['gauss'] == 0.01) & (df_watermark['alter'] == 0.01), ('quad_loss', 'mean')].to_list()\n",
    "y_splus01 = df_watermark.loc[(df_watermark['num_samples_per_class_upper_bound'] == '6stage_splus-0.1-0.1') & (df_watermark['deletion_rate'] == 0.1) & (df_watermark['error_rate'] == 0.001) & (df_watermark['gauss'] == 0.01) & (df_watermark['alter'] == 0.01), ('quad_loss', 'mean')].to_list()\n",
    "y_splus001 = df_watermark.loc[(df_watermark['num_samples_per_class_upper_bound'] == '6stage_splus-0.01-0.01') & (df_watermark['deletion_rate'] == 0.1) & (df_watermark['error_rate'] == 0.001) & (df_watermark['gauss'] == 0.01) & (df_watermark['alter'] == 0.01), ('quad_loss', 'mean')].to_list()\n",
    "y_splus0001 = df_watermark.loc[(df_watermark['num_samples_per_class_upper_bound'] == '6stage_splus-0.001-0.001') & (df_watermark['deletion_rate'] == 0.1) & (df_watermark['error_rate'] == 0.001) & (df_watermark['gauss'] == 0.01) & (df_watermark['alter'] == 0.01), ('quad_loss', 'mean')].to_list()\n",
    "\n",
    "# some do not find feasible solutions\n",
    "y_no = [y_no[0], y_no[1], y_no[2], 0]\n",
    "\n",
    "width = 0.18\n",
    "plt.bar(\n",
    "  X - width * 2.5,\n",
    "  y_no,\n",
    "  width=width,\n",
    "  label='Naive',\n",
    "  # color='#FF0000',\n",
    "  # color='#0066cc',\n",
    "  color='#7570b3',\n",
    "  hatch='xx',\n",
    "  edgecolor='k',\n",
    "  # alpha=0.5,\n",
    ")\n",
    "plt.bar(\n",
    "  X - width * 1.5,\n",
    "  y_simple01,\n",
    "  width=width,\n",
    "  label='CS, $\\\\tau=0.01$',\n",
    "  # color='#4DAF4A',\n",
    "  # color='#cc3300',\n",
    "  color='#5e81b5',\n",
    "  hatch='///',\n",
    "  edgecolor='k',\n",
    "  # alpha=0.5,\n",
    ")\n",
    "plt.bar(\n",
    "  X - width * 0.5,\n",
    "  y_simple001,\n",
    "  width=width,\n",
    "  label='CS, $\\\\tau=0.001$',\n",
    "  # color='#4DAF4A',\n",
    "  # color='#cc3300',\n",
    "  color='#71A0AC',\n",
    "  hatch='+++',\n",
    "  edgecolor='k',\n",
    "  # alpha=0.5,\n",
    ")\n",
    "plt.bar(\n",
    "  X + width * 0.5,\n",
    "  y_splus01,\n",
    "  width=width,\n",
    "  label='CS+MSO, $\\\\tau=0.1$',\n",
    "  # color='#4DAF4A',\n",
    "  # color='#cc3300',\n",
    "  color='#66c2a5',\n",
    "  hatch='---',\n",
    "  edgecolor='k',\n",
    "  # alpha=0.5,\n",
    ")\n",
    "plt.bar(\n",
    "  X + width * 1.5,\n",
    "  y_splus001,\n",
    "  width=width,\n",
    "  label='CS+MSO, $\\\\tau=0.01$',\n",
    "  # color='#4DAF4A',\n",
    "  # color='#cc3300',\n",
    "  color='#2ca25f',\n",
    "  hatch='\\\\\\\\\\\\',\n",
    "  edgecolor='k',\n",
    "  # alpha=0.5,\n",
    ")\n",
    "plt.bar(\n",
    "  X + width * 2.5,\n",
    "  y_splus0001,\n",
    "  width=width,\n",
    "  label='CS+MSO, $\\\\tau=0.001$',\n",
    "  # color='#4DAF4A',\n",
    "  # color='#cc3300',\n",
    "  color='#006d2c',\n",
    "  hatch='...',\n",
    "  edgecolor='k',\n",
    "  # alpha=0.5,\n",
    ")\n",
    "plt.legend(\n",
    "  fontsize=10, ncol=1,\n",
    "  labelspacing=0.28,         # 减小图例条目之间的垂直间距（默认0.5）\n",
    "  borderpad=0.36,\n",
    ")\n",
    "plt.xticks(ticks=X, labels=[dataname.capitalize() for dataname in all_datanames], fontsize=13)\n",
    "plt.yticks(fontsize=13)\n",
    "plt.yscale('log')\n",
    "plt.ylim(0.5, 20000)\n",
    "# plt.yticks(ticks=[0.1, 0.2, 0.3, 0.4, 0.5], labels=['0.1', '0.2', '0.3', '0.4', '0.5'], fontsize=12)\n",
    "plt.ylabel('RMSE', fontsize=14)\n",
    "plt.grid(axis='y', alpha=0.2)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabsyn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
